{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9u31JSiY-xOy"
      },
      "source": [
        "Here is the script to run the causal language model for inference in a Google Colab notebook. The code can load any of the saved types created in the Unsloth training script. Make sure to set the directory to the folder, not to the safetensors directly or any other file. Requires a CUDA-enabled GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQlidRBEBaKC"
      },
      "source": [
        "Install the dependencies necessary for Unsloth. If an error occurs despite no changes in this code block, check the Xformers version first. You may need to install an older version if you have it set to install the most recent one, as they update frequently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eSvM9zX_2d3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes #keep an eye out on the xformers version. Usually you want one version before the latest; causes errors often"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPcAVCybCRvT"
      },
      "source": [
        "First, load the LoRA adapters or model, using the same data type (dtype) as you used during training. Additionally, re-establish the alpaca_prompt variable in the same way you did during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371,
          "referenced_widgets": [
            "f170fe0bb0ef4a7caa35a239f1bc208d",
            "cd4c5150bb124210b7ef3439424333bd",
            "b43811d8621c4dbdaa23adc41de929a5",
            "9c1b37639f06485f889597f34236ca5f",
            "7e6e4671ecc64408acc08b9a36e98c5a",
            "15d6e4137ffc4e4f9718cd3a121a515f",
            "3514f59ab4b548da92173e6cde6ed3f4",
            "3134338ec31e478086a6325f24d79c0a",
            "255f277747644fceafc3be3db07b8059",
            "25c8b9ff20a34873a7161cd5701d49f6",
            "f17806c9eee442f8aa4a2e2adf3c546a",
            "0acff41eb5e048b0827a4c6853f0223a",
            "fc72b0252baa4c5db5160a58ec7402ba",
            "5a5bf2f1892c4c64becfd3603af6afd5",
            "df465c9124b94e999c101e62705a7574",
            "96b28e4b43c44b8ba9d4574622ed1724",
            "88632a6fead942de8e069dfc14eae6b3",
            "9d397b8a1b3f44e1acb2ae99d8882d2b",
            "f49f3275fb804af9aa86e2b1f4b9f0a1",
            "7ef9e3881f7f4bb4aa06b8d0ea79e3b9",
            "b7ab1a7d81984c0f906c760cdc081e9d",
            "361e8f2863d94d5692d70401d6cf426b"
          ]
        },
        "id": "MKX_XKs_BNZR",
        "outputId": "288cc4e1-2ed6-4b2a-ba96-02ab55a470f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.43.4.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.3.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f170fe0bb0ef4a7caa35a239f1bc208d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0acff41eb5e048b0827a4c6853f0223a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/172 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2024.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nfrom peft import AutoPeftModelForCausalLM\\nfrom transformers import AutoTokenizer\\nmodel = AutoPeftModelForCausalLM.from_pretrained(\\n    \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\\n    load_in_4bit = load_in_4bit,\\n)\\ntokenizer = AutoTokenizer.from_pretrained(\"lora_model\")\\n'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"/content/drive/MyDrive/Dog-LoRA\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "FastLanguageModel.for_inference(model) #Set the model for inference\n",
        "\n",
        "alpaca_prompt = \"\"\"\n",
        "\n",
        "### label:\n",
        "{}\n",
        "\n",
        "### text:\n",
        "{}\"\"\"\n",
        "\n",
        "#You can also use the Causal LM model from HF, though it is significantly slower so it is NOT recommended\n",
        "\"\"\"\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9oyLzk_DF1r"
      },
      "source": [
        "Now that the adapters and model have been loaded in the same format, you can proceed to run the model for inference using the block below. Insert your prompt under the #label section. Note that it can handle up to 128 tokens well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgfbKEbUFAOQ",
        "outputId": "0aca314e-22b3-4e11-8773-ce1873f031ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<|begin_of_text|>\\n\\n### label:\\nPlease tell me something interesting about the Rottweiler Dog\\n\\n### text:\\nRottweiler: Rottweilers are a medium to large sized dog. They are a powerful dog, but are also a very muscular dog. They have a square appearance, with a broad chest and wide shoulders. Their legs are strong, and they have a powerful tail that they carry low.<|end_of_text|>']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "labels = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Please tell me something interesting about the Rottweiler Dog\", # label\n",
        "        \"\", # text - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "texts = model.generate(**labels, max_new_tokens = 128, use_cache = True)\n",
        "tokenizer.batch_decode(texts)\n",
        "\n",
        "#The code below are some modifications you can do to play with the randomness of the responses\n",
        "#You can also add a streamer so that the tokens are loaded as they are calculated, instead of loading all of the output at once.\n",
        "\"\"\"\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "temperature = 10.0 # Must be a positive float, lower reduces randomness. This one gets wild if too high :)\n",
        "top_k = 2  # Positive integer only, no upper limit. Lower reduces randomness.\n",
        "top_p = 5  # bottom is 0, no upper limit. Lower reduces randomness.\n",
        "\n",
        "# Modify the model.generate line as follows (Add a 'streamer' so that it loads the text as it processes, instead of all at once)\n",
        "_ = model.generate(**labels, streamer=text_streamer, max_new_tokens=128, temperature=temperature, top_k=top_k, top_p=top_p, do_sample=True)\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZqkCWp_E-if"
      },
      "source": [
        "If you didn't get a chance to push to Hugging Face while training, you can do so now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187,
          "referenced_widgets": [
            "062ea9d05a974b6cada4022796dcfc5b",
            "4dcd99cb7fc549d384f0a541c933616e",
            "c7b76a56af8b43dbb1bd6689cf0b9da2",
            "72148f1b8c2d4d97a09b46e18a422250",
            "a97899b162934e5fb52407fb72d77821",
            "1a9c8f4992664c8698335c1f148a4347",
            "63a7043d930f4b5c8cb2570163ab714f",
            "2969210f54294710969150fd35865d7b",
            "137aa96ff81749e6830353999a0c8454",
            "2814347e944d4730a883b1bae9191e8b",
            "ecd9ac01b8b14013abd1a75a1ce10a75",
            "dcab2530140748fe928a7581214dfbf5",
            "fc40bf05f4784c61856ac1c3b67e2958",
            "5f7e6ea0d42c44ce85f5df6da8a83fad",
            "43e4c5e6eb3e4cccae4b0d8598b10494",
            "ac749f6909f8434c83b3ac6f606b2413",
            "7f0025aa67ee4a65a32c51185700daa6",
            "925c99c93f90418a8e4b7508489df8fc",
            "480d286caec24065ab8068402c3a43da",
            "1c4ab5913bc847c5811b486399ab604b",
            "c6d845948ffb49d6a836f631355af2b7",
            "1e4073cee3c04107831b754900c23494",
            "260badce5c2646c5a2110bca3bea8ce8",
            "94752d3a7d8347449140524146b623b6",
            "28dd925933c94b4387e06e0176e64904",
            "31f80704791c4727957db17809565529",
            "3a705e20af56425aa2e0ac64ede28eda",
            "501b581616654978928edc8a0f55e94e",
            "e9230274ee3545c1989bf3687ae719f8",
            "5ced47cf2ed644e29cb2ace71e19695e",
            "a7d8fb89fc3e45a09e000a1cac570709",
            "6162f2dcc8b94168b9ed4f06764d3ce9",
            "893d9793b91a42db9cf05914f2f16a07"
          ]
        },
        "id": "6q9YaRfvxxqN",
        "outputId": "1275156e-cfba-4c5c-b788-3d6a3d5a0f66"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "062ea9d05a974b6cada4022796dcfc5b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/578 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dcab2530140748fe928a7581214dfbf5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "260badce5c2646c5a2110bca3bea8ce8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/168M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved model to https://huggingface.co/chrismontes/Dog-LoRA\n"
          ]
        }
      ],
      "source": [
        "model.push_to_hub(\"HF_username/LoRA_adapter_name\", token = \"HF_Token\") # Online saving; you can create a token under your HF account settings. LoRA_adapter_name is the name you want to use to upload to HF.\n",
        "tokenizer.push_to_hub(\"HF_username/LoRA_adapter_name\", token = \"HF_Token\") # Same as model.push_to_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XeglNQZaBkA"
      },
      "outputs": [],
      "source": [
        "\"\"\"#If you want to use python to open Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF_EcUgZbI_t",
        "outputId": "553d7e7d-79b8-4910-89d1-82396f97e8ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/lora_model.zip\n",
            "   creating: /content/drive/MyDrive/content/lora_model/\n",
            "  inflating: /content/drive/MyDrive/content/lora_model/special_tokens_map.json  \n",
            "  inflating: /content/drive/MyDrive/content/lora_model/tokenizer_config.json  \n",
            "  inflating: /content/drive/MyDrive/content/lora_model/adapter_model.safetensors  \n",
            "  inflating: /content/drive/MyDrive/content/lora_model/tokenizer.json  \n",
            "  inflating: /content/drive/MyDrive/content/lora_model/adapter_config.json  \n",
            "  inflating: /content/drive/MyDrive/content/lora_model/README.md  \n"
          ]
        }
      ],
      "source": [
        "\"\"\"#If you zipped your adapters or model , here is a way to unzip it. -d to establish unzipped directory\n",
        "!unzip '/content/drive/MyDrive/lora_model.zip' -d '/content/drive/MyDrive/'\"\"\"\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
