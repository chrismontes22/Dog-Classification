{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbjZLk6CTr_e"
      },
      "source": [
        "Here is the full model for Google Colab. It classifies the dog breed, then based off the answer it passes that dog breed into the language model to output a naturally spoken fact about said breed. The image classification model should run on any CPU, but the language model requires a CUDA GPU. The code below also records the softmax values of the model's top 3 and the answer the user was expecting in a CSV file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT6lpNU6UT33"
      },
      "source": [
        "First you need to install the dependencies for the language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "69ThFkU0U0LT"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes #keep an eye out on the xformers version. Usually you want one version before the latest; causes errors often"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64hmGBIqUl8E"
      },
      "source": [
        "Below is the script to load and run the dog classification model. If a CUDA GPU is available, it will also load the language model here. The script is device-agnostic, so it will automatically detect a CUDA GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Q5N0F75WLiQ",
        "outputId": "7939f73c-3e7b-450c-edf1-aec77349e9d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Choose a dog breed:\n",
            "1. Saint-Bernard\n",
            "2. Basset-Hound\n",
            "3. Bluetick-Coonhound\n",
            "4. None of these.\n",
            "Enter the number of your chosen breed: 1\n",
            "My dog is a Saint Bernard breed.\n",
            "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.0.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.3.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2024.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision.transforms import v2\n",
        "import torchvision.models as models\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "# Load the pre-trained model that was used in the training script. In this case it was ResNet18 model\n",
        "model = models.resnet18()\n",
        "\n",
        "# Modify the final fully connected layer to have 73 output classes, same as in the training script\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = torch.nn.Linear(num_ftrs, 73)\n",
        "\n",
        "# Directory to load the .pth file that was acreated by the training script\n",
        "model.load_state_dict(torch.load('DIRECTORY_TO_YOUR_.PTH_FILE', map_location=torch.device('cpu')))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Automatically detect the available device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "#Open the first file in a directory with the specified filetypes\n",
        "image = Image.open(next((f for f in os.listdir('IMAGE_DIRECTORY') if f.endswith(('.jpg', '.jpeg', '.png', '.webp', 'bmp'))), None))\n",
        "\n",
        "#Transforms the images to how they were tested for the model to read for inference. Keep Exactly the same as the transformation for the test and valid sets. No randomizing here!\n",
        "transforms_test = v2.Compose([\n",
        "    v2.Resize((224, 224), antialias=True),\n",
        "    v2.CenterCrop((224, 224)),\n",
        "    v2.ToImage(),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    v2.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "#Apply the transformation to your image\n",
        "transformed_img = transforms_test(image)\n",
        "\n",
        "# Add a batch dimension (1, 3, 224, 224)\n",
        "transformed_img = transformed_img.unsqueeze(0).to(device)  # Move data to device\n",
        "\n",
        "output = model(transformed_img)\n",
        "\n",
        "labels = ['Afghan-Hound', 'Airedale-Terrier', 'Akita', 'Alaskan-Malamute', 'American-Foxhound', 'American-Hairless-Terrier', 'American-Water-Spaniel', 'Basenji', 'Basset-Hound', 'Beagle', 'Bearded-Collie', 'Belgian-Malinois', 'Belgian-Sheepdog', 'Bernese-Mountain-Dog', 'Bichon-Frise', 'Bloodhound', 'Bluetick-Coonhound', 'Border-Collie', 'Borzoi', 'Boston-Terrier', 'Boxer', 'Bull-Terrier', 'Bulldog', 'Bullmastiff', 'Cairn-Terrier', 'Cane-Corso', 'Cavalier-King-Charles-Spaniel', 'Chihuahua', 'Chinese-Crested', 'Chinese-Shar-Pei', 'Chow-Chow', 'Clumber-Spaniel', 'Cockapoo', 'Cocker-Spaniel', 'Collie', 'Dachshund', 'Dalmatian', 'Doberman-Pinscher', 'French-Bulldog', 'German-Shepherd', 'German-Shorthaired-Pointer', 'Golden-Retriever', 'Great-Dane', 'Great-Pyrenees', 'Greyhound', 'Irish-Water-Spaniel', 'Irish-Wolfhound', 'Japanese-Chin', 'Komondor', 'Labradoodle', 'Labrador-Retriever', 'Lhasa-Apso', 'Maltese', 'Miniature-Schnauzer', 'Newfoundland', 'Norwegian-Elkhound', 'Pekingese', 'Pembroke-Welsh-Corgi', 'Pomeranian', 'Poodle', 'Pug', 'Rhodesian-Ridgeback', 'Rottweiler', 'Saint-Bernard', 'Samoyed', 'Scottish-Terrier', 'Shiba-Inu', 'Shih-Tzu', 'Siberian-Husky', 'Staffordshire-Bull-Terrier', 'Vizsla', 'Xoloitzcuintli', 'Yorkshire-Terrier']\n",
        "#I have the breed_nicknames dictionary set because some breeds arent recognized that much by the official breed, such as the ones below.\n",
        "breed_nicknames = {\n",
        "    'Xoloitzcuintli': ' (Mexican Hairless)',\n",
        "    'Staffordshire-Bull-Terrier': ' (Pitbull)',\n",
        "    'Pembroke-Welsh-Corgi': ' (Corgi)',}\n",
        "\n",
        "\n",
        "# Apply softmax to get probabilities\n",
        "output_softmax = torch.nn.functional.softmax(output, dim=1)\n",
        "\n",
        "# Get the top 3 predictions by using pytorch to find the Top-3 K values\n",
        "topk_values, topk_indices = torch.topk(output_softmax, 3)\n",
        "topk_indices = topk_indices.tolist()[0]  # Convert tensor to list of integers\n",
        "topk_labels = [labels[index] for index in topk_indices] # Use the indices to get the labels. This turns the highest three values in the tensor into the labels of the dog breeds\n",
        "\n",
        "# Print all probabilities. This is useful if you would like to record all of the labels for more detailed data.\n",
        "\"\"\"for i, prob in enumerate(output_softmax.tolist()[0]):\n",
        "    print(f\"{labels[i]}: {prob:.4f}\")\"\"\"\n",
        "\n",
        "print(\"Choose a dog breed:\")\n",
        "\n",
        "early_stop = False #Set an early stop condition. Used to prevent the language model from runing later on\n",
        "\n",
        "#For data feedback, this is a function to save user data. It saves the probabilities of the Top-3, their respective labels, and the label specified by the user\n",
        "def save_answer(answer, label=None):\n",
        "    filename = \"user_answers.csv\"\n",
        "    if not os.path.exists(filename):\n",
        "        open(filename, 'w').close()  # Create file if it doesn't exist with the categories as the first row, then closes it\n",
        "        with open(filename, 'w') as file:\n",
        "            file.write(\"Probability 1, Probability 2, Probability 3, Label Rank 1, Label Rank 2, Label Rank 3, User Label, Top-3?\\n\") #The categories to be written at teh top if the file does not exist\n",
        "\n",
        "    probabilities = ', '.join(map(str, topk_values.tolist())).strip('[]')\n",
        "    labels = ', '.join(topk_labels)\n",
        "    correct_label = label if label else answer\n",
        "    top3 = 'Y' if int(answer) in range(1, 4) else 'N'  # Check if answer is in top 3\n",
        "\n",
        "    with open(filename, 'a') as file:  # Append the user results\n",
        "        file.write(f\"{probabilities}, {labels}, {correct_label}, {top3}\\n\")\n",
        "\n",
        "#While loop that repeatedly prompts user for dog breed choice until the user provides a valid response.\n",
        "while True:\n",
        "    for i, label in enumerate(topk_labels):\n",
        "        # Check if the breed has a nickname, then print it with the nickname in parantheses\n",
        "        if label in breed_nicknames:\n",
        "            print(f\"{i+1}. {label}{breed_nicknames[label]}\")\n",
        "        else:\n",
        "            print(f\"{i+1}. {label}\")\n",
        "    print(\"4. None of these.\")\n",
        "    try:\n",
        "        choice = int(input(\"Enter the number of your chosen breed: \")) - 1\n",
        "        if choice in [0, 1, 2]:\n",
        "            dog_breed = topk_labels[choice]\n",
        "            print(f\"My dog is a {dog_breed.replace('-', ' ')} breed.\")\n",
        "            save_answer(choice + 1, dog_breed)\n",
        "            break\n",
        "        elif choice == 3:  # If the dog breed is not in the top-3, the following instead is ran\n",
        "            dog_breed = input(\"Please enter your dog's breed (50 characters or less): \")\n",
        "            if len(dog_breed) <= 50: #Allows the user to only input 50 characters\n",
        "                early_stop = True #This blocks the language model from operating when a manual input is inserted. Set to False if you want the language model to work on manual input\n",
        "                print(\"Thank you for letting us know! We'll work on improving our model for\", dog_breed, \"breeds.\")\n",
        "                save_answer(4, dog_breed)\n",
        "                break\n",
        "            else:\n",
        "                print(\"Sorry, that's too long. Please keep it under 50 characters.\")\n",
        "        else:\n",
        "            print(\"Invalid choice. Please enter 1, 2, 3, or 4.\")\n",
        "    except ValueError:\n",
        "        print(\"Invalid input. Please enter a number.\")\n",
        "\n",
        "#Major Note: If you would like for the custom breed (ie. they choose option 4) to be used in the language model, set the early_stop variable that is INSIDE the loop to False.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Load the LoRA adapters and set FastLanguageModel for inference (if a CUDA GPU is present)\n",
        "#Won't load if you answered number 4 and you DID NOT change early_stop to True in the while block\n",
        "if not early_stop:\n",
        "  if torch.cuda.is_available():\n",
        "      from unsloth import FastLanguageModel\n",
        "\n",
        "\n",
        "      max_seq_length = 2048\n",
        "      dtype = None\n",
        "      load_in_4bit = True\n",
        "\n",
        "      model, tokenizer = FastLanguageModel.from_pretrained( #same parameters as it was trained on.\n",
        "          model_name = \"DIRECTORY_TO_LANGUAGE_MODEL\", #Directory to the folder (not the file) where your model is saved. Any of the save methods from the Unsloth training should work\n",
        "          max_seq_length = max_seq_length,\n",
        "          dtype = dtype,\n",
        "          load_in_4bit = load_in_4bit,\n",
        "      )\n",
        "      FastLanguageModel.for_inference(model)\n",
        "\n",
        "      #Need to set the prompt again\n",
        "      alpaca_prompt = \"\"\"\n",
        "\n",
        "      ### label:\n",
        "      {}\n",
        "\n",
        "      ### text:\n",
        "      {}\"\"\"\n",
        "\n",
        "  else:\n",
        "      print(\"Language model output is only available for GPU hardware\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Zp1Au4GyOVZ"
      },
      "source": [
        "If you loaded the language model in the previouos cell, the next cell is for running the dog_breed variable through the model. It is in a seperate cell so you may run the output several times without reloading the output each time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXbi_oPFZ0EB",
        "outputId": "483a04c2-84fc-429e-a20e-0653c88fbdfb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<|begin_of_text|>\\n\\n      ### label:\\n      Please tell me something interesting about the Saint-Bernard Dog\\n\\n      ### text:\\n       Saint-Bernard: The Saint Bernard is a large, powerful dog. Males stand 26 to 28 inches at the shoulder and weigh 120 to 180 pounds. Females are slightly smaller.<|end_of_text|>']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        f\"Please tell me something interesting about the {dog_breed} Dog\", # label\n",
        "        \"\", # text - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "texts = model.generate(**labels, max_new_tokens = 128, use_cache = True)\n",
        "tokenizer.batch_decode(texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXYbvReQY3Pa"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "OH1KEk74Uptn",
        "outputId": "267945b6-fca8-40e5-ef14-df16ad691f4d"
      },
      "outputs": [
        {
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "#I put it on the bottom, but here is a way to connect to your Google Drive with python. You can also do it manually (its actually faster manually) under Files in the Colab sidebar\n",
        "\"\"\"from google.colab import drive\n",
        "drive.mount('/content/drive')\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjhXcgOiU00s"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4BlfJ5-T0C-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
