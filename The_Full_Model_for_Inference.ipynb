{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Here is the full model for Google Colab. It classifies the dog breed on an image, then based off the answer it passes that dog breed into the language model to output a naturally spoken fact about said breed. The image classification model should run on any CPU, but the language model requires a CUDA GPU. The code below also records the softmax values of the model's top 3 and the answer the user was expecting in a CSV file."
      ],
      "metadata": {
        "id": "EbjZLk6CTr_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='red'>First you need to install the dependencies for the language model. You can do so by running the cell bellow. If you do not have a CUDA GPU, you can skip this cell. Given that you are most likely doing this in Google Colab, you can access a GPU by clicking on the triangle pointing downward under \"Comment\" in the top left corner, then going to \"Change Runtime Type\" and choosing the \"T4 GPU\"."
      ],
      "metadata": {
        "id": "pT6lpNU6UT33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install xformers trl peft accelerate triton bitsandbytes #keep an eye out on the xformers version. Usually you want one version before the latest; causes errors often"
      ],
      "metadata": {
        "id": "69ThFkU0U0LT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will need LoRA adapters and a pytorch dictionary file (.pth) for image calssification. The cell below downloads them from a Google Drive. The last four lines download a couple of images to test around as well.\n",
        "\n",
        "<font color='red'>Run the cell to get the adapters and pytorch dictionary. If you have your own images of a dog instead feel free to erase the last four lines here."
      ],
      "metadata": {
        "id": "OeILsjvwgl2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "#Download the folder containnng the LoRA adapters\n",
        "url = \"https://drive.google.com/drive/folders/1cf-XTMMZb42k5bs3AR3xMRs9V5koXZnQ\"\n",
        "gdown.download_folder(url, quiet=True, use_cookies=False)\n",
        "\n",
        "#Download the .pth image model\n",
        "url = 'https://drive.google.com/file/d/1ABhX5RLxh_-OSubYhwV7RhgqeeaHYOpl/view?usp=sharing'\n",
        "output_path = 'DogImageModel.pth'\n",
        "gdown.download(url, output_path, quiet=False,fuzzy=True)\n",
        "\n",
        "#Download the Dog_List.txt\n",
        "url = 'https://drive.google.com/file/d/13Va0eqByu_CiR1O3xXO-OZLfJjY8NwwM/view?usp=sharing'\n",
        "output_path = 'Dog_List.txt'\n",
        "gdown.download(url, output_path, quiet=False,fuzzy=True)\n",
        "\n",
        "#Download some sample images to try the model on\n",
        "gdown.download('https://drive.google.com/file/d/1PLTSwWyPoMwgxBU9gTwDoYSzokqAJdVF/view?usp=sharing', 'Dogpic1.jpg', quiet=False,fuzzy=True)\n",
        "gdown.download('https://drive.google.com/file/d/1LDp6ru_AhUQd4pHecIBMqyiw2tQdKpfe/view?usp=sharing', 'Dogpic2.jpg', quiet=False,fuzzy=True)\n",
        "gdown.download('https://drive.google.com/file/d/1uA2HJknMZyeR8X3vVvJgTq8pJAmCzaCp/view?usp=sharing', 'Dogpic3.jpg', quiet=False,fuzzy=True)\n",
        "gdown.download('https://drive.google.com/file/d/1kY5cxn7bTtNm2waDn9hPxg6E5rC7UZYS/view?usp=sharing', 'Dogpic4.jpg', quiet=False,fuzzy=True)"
      ],
      "metadata": {
        "id": "5VLagOTr_b4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Down below is the script to load and run the dog classification model. If a CUDA GPU model is available, it will also load the language model here. It is device agnostic so it will automatically detect CUDA.\n",
        "\n",
        "<font color='red'>Focusing on the image_path variable, feel free  to use one of the images provided by the download above (Dogpic1.jpg - Dogpic.jpg4). You can also use your own photo of a dog from the selected dog breed list. If you choose to use your own photo, make sure to upload it and adjust the image_path to wherever your photo is saved in the Colab files (Google Colab ownly allows temporary uploads so it will be erased at the end of your runtime). You might need to change the extension to .jpg before uploading. After you have uploaded and adjusted the image_path variable appropriately, you can run the cell."
      ],
      "metadata": {
        "id": "64hmGBIqUl8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Specify the path to your image here in the Google Colab notebook. You can copy the path by right clicking on the file you want to use. Usually an uploaded image is found in the /content folder\n",
        "image_path = '/content/Dogpic1.jpg'\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision.transforms import v2\n",
        "import torchvision.models as models\n",
        "import os\n",
        "\n",
        "# Load the pre-trained model that was used in the training script. In this case it was ResNet18 model\n",
        "model = models.resnet18()\n",
        "\n",
        "# Modify the final fully connected layer to have 73 output classes, same as in the training script\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = torch.nn.Linear(num_ftrs, 73)\n",
        "\n",
        "# Directory to load the .pth file that was acreated by the training script\n",
        "model.load_state_dict(torch.load('/content/DogImageModel.pth', map_location=torch.device('cpu')))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Automatically detect the available device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "#Open a specified image\n",
        "image = Image.open(image_path)\n",
        "\n",
        "#Open the first file in a folder directory with the specified filetypes\n",
        "\"\"\"image = Image.open(next((f for f in os.listdir('/content/') if f.endswith(('.jpg', '.jpeg', '.png', '.webp', 'bmp'))), None))\"\"\"\n",
        "\n",
        "#Transforms the images to how they were tested for the model to read for inference. Keep Exactly the same as the transformation for the test and valid sets. No randomizing here!\n",
        "transforms_test = v2.Compose([\n",
        "    v2.Resize((224, 224), antialias=True),\n",
        "    v2.CenterCrop((224, 224)),\n",
        "    v2.ToImage(),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    v2.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "#Apply the transformation to your image\n",
        "transformed_img = transforms_test(image)\n",
        "\n",
        "# Add a batch dimension (1, 3, 224, 224)\n",
        "transformed_img = transformed_img.unsqueeze(0).to(device)  # Move data to device\n",
        "\n",
        "output = model(transformed_img)\n",
        "\n",
        "with open('Dog_List.txt', 'r') as f:\n",
        "    labels = [line.strip() for line in f.readlines()]\n",
        "\n",
        "#I have the breed_nicknames dictionary set because some breeds arent recognized that much by the official breed, such as the ones below.\n",
        "breed_nicknames = {\n",
        "    'Xoloitzcuintli': ' (Mexican Hairless)',\n",
        "    'Staffordshire-Bull-Terrier': ' (Pitbull)',\n",
        "    'Pembroke-Welsh-Corgi': ' (Corgi)',}\n",
        "\n",
        "\n",
        "# Apply softmax to get probabilities\n",
        "output_softmax = torch.nn.functional.softmax(output, dim=1)\n",
        "\n",
        "# Get the top 3 predictions by using pytorch to find the Top-3 K values\n",
        "topk_values, topk_indices = torch.topk(output_softmax, 3)\n",
        "topk_indices = topk_indices.tolist()[0]  # Convert tensor to list of integers\n",
        "topk_labels = [labels[index] for index in topk_indices] # Use the indices to get the labels. This turns the highest three values in the tensor into the labels of the dog breeds\n",
        "\n",
        "# Print all probabilities. This is useful if you would like to record all of the labels for more detailed data.\n",
        "\"\"\"for i, prob in enumerate(output_softmax.tolist()[0]):\n",
        "    print(f\"{labels[i]}: {prob:.4f}\")\"\"\"\n",
        "\n",
        "print(\"Choose a dog breed:\")\n",
        "\n",
        "early_stop = False #Set an early stop condition. Used to prevent the language model from runing later on\n",
        "\n",
        "#For data feedback, this is a function to save user data. It saves the probabilities of the Top-3, their respective labels, and the label specified by the user\n",
        "def save_answer(answer, label=None):\n",
        "    filename = \"user_answers.csv\"\n",
        "    if not os.path.exists(filename):\n",
        "        open(filename, 'w').close()  # Create file if it doesn't exist with the categories as the first row, then closes it\n",
        "        with open(filename, 'w') as file:\n",
        "            file.write(\"Probability 1, Probability 2, Probability 3, Label Rank 1, Label Rank 2, Label Rank 3, User Label, Top-3?\\n\") #The categories to be written at teh top if the file does not exist\n",
        "\n",
        "    probabilities = ', '.join(map(str, topk_values.tolist())).strip('[]')\n",
        "    labels = ', '.join(topk_labels)\n",
        "    correct_label = label if label else answer\n",
        "    top3 = 'Y' if int(answer) in range(1, 4) else 'N'  # Check if answer is in top 3\n",
        "\n",
        "    with open(filename, 'a') as file:  # Append the user results\n",
        "        file.write(f\"{probabilities}, {labels}, {correct_label}, {top3}\\n\")\n",
        "\n",
        "#While loop that repeatedly prompts user for dog breed choice until the user provides a valid response.\n",
        "while True:\n",
        "    for i, label in enumerate(topk_labels):\n",
        "        # Check if the breed has a nickname, then print it with the nickname in parantheses\n",
        "        if label in breed_nicknames:\n",
        "            print(f\"{i+1}. {label}{breed_nicknames[label]}\")\n",
        "        else:\n",
        "            print(f\"{i+1}. {label}\")\n",
        "    print(\"4. None of these.\")\n",
        "    try:\n",
        "        choice = int(input(\"Enter the number of your chosen breed: \")) - 1\n",
        "        if choice in [0, 1, 2]:\n",
        "            dog_breed = topk_labels[choice]\n",
        "            print(f\"My dog is a {dog_breed.replace('-', ' ')} breed.\")\n",
        "            save_answer(choice + 1, dog_breed)\n",
        "            break\n",
        "        elif choice == 3:  # If the dog breed is not in the top-3, the following instead is ran\n",
        "            dog_breed = input(\"Please enter your dog's breed (50 characters or less): \")\n",
        "            if len(dog_breed) <= 50: #Allows the user to only input 50 characters\n",
        "                early_stop = True #This blocks the language model from operating when a manual input is inserted. Set to False if you want the language model to work on manual input\n",
        "                print(\"Thank you for letting us know! We'll work on improving our model for\", dog_breed, \"breeds.\")\n",
        "                save_answer(4, dog_breed)\n",
        "                break\n",
        "            else:\n",
        "                print(\"Sorry, that's too long. Please keep it under 50 characters.\")\n",
        "        else:\n",
        "            print(\"Invalid choice. Please enter 1, 2, 3, or 4.\")\n",
        "    except ValueError:\n",
        "        print(\"Invalid input. Please enter a number.\")\n",
        "\n",
        "#Major Note: If you would like for the custom breed (ie. they choose option 4) to be used in the language model, set the early_stop variable that is INSIDE the loop to False.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Load the LoRA adapters and set FastLanguageModel for inference (if a CUDA GPU is present)\n",
        "#Wont load if the user answered number 4 and you DID NOT change early_stop to True in the while block\n",
        "if not early_stop:\n",
        "  if torch.cuda.is_available():\n",
        "      from unsloth import FastLanguageModel\n",
        "\n",
        "\n",
        "      max_seq_length = 2048\n",
        "      dtype = None\n",
        "      load_in_4bit = True\n",
        "\n",
        "      model, tokenizer = FastLanguageModel.from_pretrained( #same parameters as it was trained on.\n",
        "          model_name = \"/content/Dog-LoRA\", #Directory to the folder (not the file) where your model is saved. Any of the save methods from the Unsloth training should work\n",
        "          max_seq_length = max_seq_length,\n",
        "          dtype = dtype,\n",
        "          load_in_4bit = load_in_4bit,\n",
        "      )\n",
        "      FastLanguageModel.for_inference(model)\n",
        "\n",
        "      #Need to set the prompt again\n",
        "      alpaca_prompt = \"\"\"\n",
        "\n",
        "      ### label:\n",
        "      {}\n",
        "\n",
        "      ### text:\n",
        "      {}\"\"\"\n",
        "\n",
        "      labels = tokenizer(\n",
        "          [\n",
        "              alpaca_prompt.format(\n",
        "                  f\"Please tell me something interesting about the {dog_breed} Dog\",\n",
        "                  \"\",\n",
        "              )\n",
        "          ], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "\n",
        "  else:\n",
        "      print(\"Language model output is only available for GPU hardware\")\n",
        "\n",
        "try:\n",
        "    texts = model.generate(**labels, max_new_tokens = 128, use_cache = True)\n",
        "    print(tokenizer.batch_decode(texts))\n",
        "except Exception as e:\n",
        "    pass  # Ignore the error\n",
        "\n"
      ],
      "metadata": {
        "id": "5Q5N0F75WLiQ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you loaded the language model in the previouos cell, the next cell is for running the dog_breed variable through the model. It is in a seperate cell so you may run the output several times without reloading the output each time."
      ],
      "metadata": {
        "id": "6Zp1Au4GyOVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = model.generate(**labels, max_new_tokens = 128, use_cache = True)\n",
        "tokenizer.batch_decode(texts)"
      ],
      "metadata": {
        "id": "TXbi_oPFZ0EB",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}